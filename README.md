# python_tools

递归删除文件夹中的重复文件

- 递归扫描指定文件夹中的所有文件
- 使用 SHA256 算法计算每个文件的哈希值
- 识别内容完全相同的文件（基于哈希值比较）
- 显示重复文件的详细信息，包括文件路径和大小
- 提供自动删除选项，保留每组重复文件中的第一个文件



分块读取机制 ：脚本使用了分块读取的方式计算文件哈希值，而不是一次性将整个文件加载到内存中：

```
def calculate_file_hash(file_path, block_size=65536):
    sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            for block in iter(lambda: f.read(block_size), b''):
                sha256.update(block)
        return sha256.hexdigest()
```

默认块大小 ：脚本使用65536字节(64KB)作为默认块大小，这意味着无论文件多大，它每次只会读取64KB到内存中，因此不会因为文件大小而耗尽内存。



## 优化建议
如果需要处理大量大文件，可以考虑以下优化：

1. 增加块大小 ：可以适当增加 block_size 参数值，如改为1MB或更大，以减少I/O操作次数。
2. 添加进度指示器 ：对于大文件，添加更详细的进度指示器会更有帮助。
3. 并行处理 ：可以考虑使用多线程或多进程来并行处理多个文件。
4. 预先检查文件大小 ：在计算哈希值前先检查文件大小，对特别大的文件可以采用不同的策略。





































